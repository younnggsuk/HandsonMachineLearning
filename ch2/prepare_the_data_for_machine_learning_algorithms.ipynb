{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞 절에서 수행한 것들\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "dataset_root = os.path.join(os.getcwd(), \"datasets\")\n",
    "housing_path = os.path.join(dataset_root, \"housing\")\n",
    "\n",
    "def load_housing_data(housing_path=housing_path):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "housing = load_housing_data()\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1,\n",
    "                               test_size=0.2,\n",
    "                               random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "머신러닝 알고리즘을 위한 데이터를 준비해본다. 이는 수동으로 작업하기보다, 함수로 작성하는 것이 더욱 좋다. 그 이유는 다음과 같다.\n",
    "- 다른 dataset에도 동일한 변환을 재사용 할 수 있다.\n",
    "- 향후 프로젝트에 재사용할 변환 함수들의 라이브러리를 점진적으로 구축하게 된다.\n",
    "- live system의 알고리즘에 새로운 데이터를 주입하기 전에 변환을 하기 위해 이 함수를 사용할 수있다.\n",
    "- 다양한 변환들을 쉽게 시도하고, 가장 잘 동작하는 변환들의 조합을 확인할 수 있게 만들어 줄 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`strat_train_set`으로 다시 되돌아가서, training set에서 변환이 일어나면 안되는 target을 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "여기서는 missing feature(`total_bedrooms`의 non-null value)를 처리하는 함수를 만들어본다. 이는 다음과 같은 3가지 방법으로 수행할 수 있다.\n",
    "1. missing feature인 district를 모두 제거한다.\n",
    "2. 해당 feature 전체를 제거한다.\n",
    "3. 직접 값을 설정해준다.(0, mean, median 등)\n",
    "\n",
    "위의 3가지 방법은 DataFrame의 `dropna()`, `drop()`, `fillna()`를 통해 다음과 같이 쉽게 수행할 수 있다.\n",
    "\n",
    "    # 1번째 방법\n",
    "    housing.dropna(subset=[\"total_bedrooms\"])\n",
    "\n",
    "    # 2번째 방법\n",
    "    housing.drop(\"total_bedrooms\", axis=1)\n",
    "\n",
    "    # 3번째 방법(여기서는 median으로 한 예)\n",
    "    median = housing[\"total_bedrooms\"].median()\n",
    "    housing[\"total_bedrooms\"].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약, 3번째 방법으로 수행한다면, 반드시 training set에서 계산한 값으로 설정해야한다. 또한, 이후 evaluation에 사용되는 test set과 시스템의 운영시에 새로운 데이터의 missing value에도 이 값을 사용해야 하므로 반드시 저장해놓아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런의 `SimpleImputer` class를 사용하면 missing value를 쉽게 다룰 수 있다. 먼저, 다음과 같이 `strategy`를 constructor의 파라미터로 전달하며 `SimpleImputer` instance를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "median 값은 수치 값에서만 계산될 수 있으므로, text feature인 `ocean_proximity`를 제거해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 위 수치 값으로 이루어진 데이터를 인자로 넘겨주며 `fit()`을 해주게되면, imputer instance는 각 feature별로 median을 계산해서 `statistics_` instance 변수에 저장한다.\n",
    "\n",
    "현재의 데이터에는 `total_bedrooms`에만 missing value가 있지만, 새로운 데이터에는 어떤 값이 missing될지 모르므로, 모든 feature에 대해 계산해놓는 것이 안전하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-118.51     34.26     29.     2119.5     433.     1164.      408.\n",
      "    3.5409]\n",
      "[-118.51     34.26     29.     2119.5     433.     1164.      408.\n",
      "    3.5409]\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(housing_num)\n",
    "print(imputer.statistics_)\n",
    "\n",
    "# median값이 계산이 잘 되었는지 확인\n",
    "print(housing_num.median().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 위의 학습된 `imputer`를 통해 training set의 missing value를 median값으로 대체할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transform()`의 결과는 numpy array형태이다. 다시 pandas의 DataFrame으로 바꾸고 싶다면 다음과 같이 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.575834</td>\n",
       "      <td>35.639577</td>\n",
       "      <td>28.653101</td>\n",
       "      <td>2622.728319</td>\n",
       "      <td>533.998123</td>\n",
       "      <td>1419.790819</td>\n",
       "      <td>497.060380</td>\n",
       "      <td>3.875589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.001860</td>\n",
       "      <td>2.138058</td>\n",
       "      <td>12.574726</td>\n",
       "      <td>2138.458419</td>\n",
       "      <td>410.839621</td>\n",
       "      <td>1115.686241</td>\n",
       "      <td>375.720845</td>\n",
       "      <td>1.904950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.350000</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.499900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.800000</td>\n",
       "      <td>33.940000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1443.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>784.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>2.566775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.510000</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2119.500000</td>\n",
       "      <td>433.000000</td>\n",
       "      <td>1164.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>3.540900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.010000</td>\n",
       "      <td>37.720000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3141.000000</td>\n",
       "      <td>641.000000</td>\n",
       "      <td>1719.250000</td>\n",
       "      <td>602.000000</td>\n",
       "      <td>4.744475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.310000</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>39320.000000</td>\n",
       "      <td>6210.000000</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>5358.000000</td>\n",
       "      <td>15.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  16512.000000  16512.000000        16512.000000  16512.000000   \n",
       "mean    -119.575834     35.639577           28.653101   2622.728319   \n",
       "std        2.001860      2.138058           12.574726   2138.458419   \n",
       "min     -124.350000     32.540000            1.000000      6.000000   \n",
       "25%     -121.800000     33.940000           18.000000   1443.000000   \n",
       "50%     -118.510000     34.260000           29.000000   2119.500000   \n",
       "75%     -118.010000     37.720000           37.000000   3141.000000   \n",
       "max     -114.310000     41.950000           52.000000  39320.000000   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \n",
       "count    16512.000000  16512.000000  16512.000000   16512.000000  \n",
       "mean       533.998123   1419.790819    497.060380       3.875589  \n",
       "std        410.839621   1115.686241    375.720845       1.904950  \n",
       "min          2.000000      3.000000      2.000000       0.499900  \n",
       "25%        296.000000    784.000000    279.000000       2.566775  \n",
       "50%        433.000000   1164.000000    408.000000       3.540900  \n",
       "75%        641.000000   1719.250000    602.000000       4.744475  \n",
       "max       6210.000000  35682.000000   5358.000000      15.000100  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(X))\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n",
    "print(type(housing_tr))\n",
    "housing_tr.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Text and Categorical Attributes\n",
    "\n",
    "이제 text로 구성된 categorical feature인 `ocean_proximity`를 처리해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17606</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18632</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14650</th>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3555</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19480</th>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8879</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13685</th>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4937</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ocean_proximity\n",
       "17606       <1H OCEAN\n",
       "18632       <1H OCEAN\n",
       "14650      NEAR OCEAN\n",
       "3230           INLAND\n",
       "3555        <1H OCEAN\n",
       "19480          INLAND\n",
       "8879        <1H OCEAN\n",
       "13685          INLAND\n",
       "4937        <1H OCEAN\n",
       "4861        <1H OCEAN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 머신러닝 알고리즘은 숫자를 다루므로, text를 숫자로 바꾸어준다. 다음과 같이 사이킷런의 `OrdinaryEncoder` class를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [4.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "print(housing_cat_encoded[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "category들의 목록은 `categories_` instance 변수를 통해 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
      "      dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "print(ordinal_encoder.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 방법의 문제는 머신러닝 알고리즘이 가까운 값을 먼 값보다 비슷하다고 생각한다는 것이다. 예를 들어, 0과 1은 0과 4보다 유사하다고 판단한다.\n",
    "\n",
    "이를 해결하기 위한 일반적인 솔루션은 각 category별 binary 값으로 표현하는 것이다. 이 방법은 **one-hot encoding**이라고 한다.\n",
    "\n",
    "사이킷런에서는 `OneHotEncoder` class를 통해 categorical value를 one-hot vector로 표현할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16512x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16512 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 주목할 점은 출력이 numpy array가 아닌 scipy sparse matrix라는 것이다. 이는 매우 많은 category를 가지는 categorical feature에서 매우 유용하다. 0과 1로만 이루어진 아주 큰 matrix는 메모리 낭비가 심하므로, 1의 위치만을 가지는 sparse matrix 형태로 저장하는 것이다.\n",
    "\n",
    "만약 이 sparse matrix형태를 원래의 dense matrix(numpy array)로 변환하려면, 다음과 같이 `toarray()`를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformers\n",
    "\n",
    "사이킷런에서 직접 custom transformer를 만들기 위해서는 3가지 methods:`fit()`, `transform()`, `fit_transform()`가 구현된 class만 작성하면 된다.\n",
    "\n",
    "또한 `fit_transform()`은 `TransformerMixin`  class만 상속해주면 자동으로 구현되며, `BaseEstimator` class를 추가로 상속할 경우, hyperparameter tuning의 자동화에 유용한 2가지 methods:`get_params()`와 `set_params()`가 구현된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 combined feature를 추가하는 transformer를 만들어본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_idx, bedrooms_idx, population_idx, households_idx = 3, 4, 5, 6\n",
    "\n",
    "class CombinedFeaturesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Nothing else to do\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_idx]/X[:, households_idx]\n",
    "        population_per_household = X[:, population_idx]/X[:, households_idx]\n",
    "        # bedrooms_per_room은 선택\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_idx]/X[:, rooms_idx]\n",
    "            return np.c_[X,\n",
    "                         rooms_per_household,\n",
    "                         population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X,\n",
    "                         rooms_per_household,\n",
    "                         population_per_household]\n",
    "    \n",
    "\n",
    "feature_adder = CombinedFeaturesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_features = feature_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예제에서는 `add_bedrooms_per_room` hyperparameter를 하나 가지고 있으며 기본으로 `True`를 설정했다.(합리적인 기본 값을 설정하는 것이 좋음)\n",
    "\n",
    "이처럼 100% 확신이 없는 데이터에 대해 hyperparameter를 추가하면, 더욱 많은 combination을 자동으로 시도할 수 있게 되며, 최상의 combination을 찾게 될 가능성을 높여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "몇가지 경우를 제외하고, 머신러닝 알고리즘은 서로 다른 scale의 수치 feature에서 좋은 성능을 내지 못하므로 feature scaling은 아주 중요하다.\n",
    "\n",
    "모든 feature가 같은 scale을 가지도록 하는 일반적인 방법은 다음의 2가지가 있다.\n",
    "\n",
    "#### 1. min-max scaling\n",
    "\n",
    "min-max scaling은 다음의 식을 통해 수행된다.\n",
    "- $x = \\dfrac{x-x_{min}}{x_{max}-x_{min}}$\n",
    "\n",
    "min-max scaling은 값을 0에서 1사이의 범위로 만들어준다.\n",
    "\n",
    "사이킷런에서는 `MinMaxScaler`를 통해 수행할 수 있으며, `feature_range` hyperparameter를 통해 0에서 1이 아닌 다른 범위를 지정할 수도 있다.\n",
    "\n",
    "#### 2. standardization\n",
    "\n",
    "Standardization은 다음의 식을 통해 수행된다.\n",
    "- $x = \\dfrac{x-x_{mean}}{x_{std}}$\n",
    "\n",
    "standardization을 수행하면, 평균이 0이고 분산이 1인 분포를 가지게 되며 값의 범위가 정해지지는 않는다. 또한, standardization은 min-max scaling에 비해 outlier에 덜 민감하다는 특징이 있다.\n",
    "\n",
    "사이킷런에서는 `StandardScaler`를 통해 수행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고\n",
    "지금까지 살펴본 모든 transformer는 training set에서 값을 구한다는 것을 기억해야한다.\n",
    "\n",
    "즉, training set에 `fit()`을 한 후, 모든 데이터(train, test, new data 등)에 `transform()`한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Pipelines\n",
    "\n",
    "사이킷런의 `Pipeline` class를 사용하면 다음과 같이 여러 변환 sequence를 올바른 순서로 실행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 앞서 해왔던 변환들을 하나의 pipeline으로 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('features_adder', CombinedFeaturesAdder()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipeline` constructor는 (이름, estimator)쌍으로 이루어진 list를 받아 sequence를 정의한다. 또한, 여기서 정의된 이름은 나중에 hyperparameter tuning에서 사용된다.\n",
    "\n",
    "pipeline의 `fit()`을 호출하면, estimator들은 순서대로 `fit_transform()`을 호출하며 마지막 estimator는 `fit()`만 호출한다. \n",
    "\n",
    "따라서, 마지막 estimator는 `fit()`, 나머지 estimator들은 `fit()`, `transform()` 또는 `fit_transform()`까지 포함해서 가지고 있어야 한다. (`fit()`, `transform()`만 있는 경우, `fit_transform()`을 호출하면, 둘을 차례로 실행한다.)\n",
    "\n",
    "pipeline은 마지막 estimator의 method를 제공한다. 여기서는 `StandardScaler`에 `fit()` ,`transform()`이 존재하므로, `fit_transform()`을 사용할 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런의 `ColumnTransformer`는 각 column별로 처리해야 할 여러 변환들을 하나로 묶어준다. 이를 사용하면 다음과 같이 categorical feature와 numerical feature를 처리하는 변환을 하나로 묶을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_features = list(housing_num)\n",
    "cat_features = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_features),\n",
    "    (\"cat\", OneHotEncoder(), cat_features),\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
