{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 차원 축소를 위한 접근 방법\n",
    "\n",
    "### 차원을 감소시키는 2가지 주요한 접근법\n",
    "- 투영 (Projection)\n",
    "- 매니폴드 학습 (Manifold Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 투영 (Projection)\n",
    "\n",
    "대부분의 훈련 샘플들은 모든 차원에 균일하게 퍼져있지 않으며, 고차원 공간의 부분 공간(subspace)에 위치하게 된다. 이러한 경우, 투영을 사용해 해당 부분 공간으로 차원을 줄일 수 있다.\n",
    "\n",
    "예를 들어, 다음과 같은 3차원 공간에서, 2차원 평면에 데이터가 집중적으로 분포한 경우,\n",
    "\n",
    "<img src=\"./images/projection_3d.png\" alt=\"projection_3d\"  width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같이 2차원 공간으로 투영을 할 수 있다.\n",
    "\n",
    "<img src=\"./images/projection_2d.png\" alt=\"projection_2d\"  width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 매니폴드 학습 (Manifold Learning)\n",
    "\n",
    "### 매니폴드 (Manifold)\n",
    "\n",
    "매니폴드(manifold)란, 아래 그림과 같이 국소적으로는 유클리디안을 따르지만 대역적으로는 그렇지 않은 공간을 의미한다.\n",
    "<img src=\"./images/manifold.png\" alt=\"manifold\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 그림은 스위스 롤(swiss roll)이라 불리는 2D 매니폴드의 한 형태이다.\n",
    "\n",
    "<img src=\"./images/swiss_roll.png\" alt=\"swiss_roll\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서, d차원 매니폴드란, 국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부를 의미하며, 스위스 롤의 경우, d=2, n=3인 경우이다.\n",
    "\n",
    "\n",
    "### 매니폴드 학습 (Manifold learning)\n",
    "매니폴드 학습이란, 훈련 샘플이 놓여있는 매니 폴드를 모델링하는 차원 축소 알고리즘을 의미하며, 이는 대부분의 고차원 데이터셋이 저차원 매니폴드에 더 가깝게 놓여있다는 매니폴드 가정(manifold assumption) 또는 매니폴드 가설(manifold hypothesis)에 근거한다.\n",
    "\n",
    "앞서 살펴본 스위스 롤은, 매니폴드 학습을 통해 다음과 같이 펼쳐 2차원으로 차원을 축소할 수 있다.\n",
    "\n",
    "<img src=\"./images/unrolling_swiss_roll.png\" alt=\"unrolling_swiss_roll\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매니폴드 가정은 종종 저차원의 매니폴드 공간에 표현할 경우, 더욱 간단해 질 것이라는 암묵적인 가정과 병해되는 경우가 있는데, 하지만 이러한 가정은 항상 유효하지 않다.\n",
    "\n",
    "다음의 그림에서, 위의 경우는 유효하였지만, 아래의 경우는 그렇지 않다는 것을 알 수 있다.\n",
    "\n",
    "<img src=\"./images/lower_dimensions_are_not_always_simpler.png\" alt=\"lower_dimensions_are_not_always_simpler\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요약\n",
    "\n",
    "Projection, Manifold Learning과 같은 차원 축소는 훈련 속도를 빠르게 하지만, 항상 더 좋은 솔루션(결과)을 가져오지는 않으며, 이는 데이터셋에 따라 다르다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
