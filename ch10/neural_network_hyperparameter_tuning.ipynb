{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 하이퍼파라미터 튜닝하기\n",
    "\n",
    "케라스 모델을 사이킷런 추정기처럼 동작하도록 하면, 2장에서와 같이 사이킷런의 `GridSearchCV`나 `RandomizedSearchCV`를 통해 가장 좋은 점수를 내는 하이퍼파라미터 조합을 k-fold cross validation을 통해 찾을 수 있다.\n",
    "\n",
    "캘리포니아 주택 가격 데이터셋을 이용한 회귀 모델을 통해 이를 수행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "import os\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_path = os.path.join(os.getcwd(), \"datasets\")\n",
    "\n",
    "if not os.path.isdir(dataset_path):\n",
    "    os.makedirs(dataset_path)\n",
    "\n",
    "housing = fetch_california_housing(data_home=dataset_path)\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full,\n",
    "                                                  y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, 하이퍼파라미터로 케라스 모델을 만들고 컴파일 하는 함수를 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=optimizers.SGD(lr=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KerasRegressor` 객체는 케라스 모델을 감싸는 간단한 래퍼이다. 이를 사용하면, 사이킷런 회귀 추정기처럼 이 객체를 사용할 수 있다.\n",
    "\n",
    "`build_model()`에 정의된 기본 하이퍼파라미터를 사용하는 모델을 `KerasRegressor` 객체로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = tf.keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `fit()`, `score()`, `predict()`를 사용하는 사이킷런처럼 이 객체를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 1.1269 - val_loss: 0.6646\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5889 - val_loss: 0.5385\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5168 - val_loss: 0.4913\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4899 - val_loss: 0.4691\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4694 - val_loss: 0.4576\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4573 - val_loss: 0.4443\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4484 - val_loss: 0.4363\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4413 - val_loss: 0.4300\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4310 - val_loss: 0.4227\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4229 - val_loss: 0.4175\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.4184 - val_loss: 0.4138\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4126 - val_loss: 0.4102\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.4067 - val_loss: 0.4046\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4032 - val_loss: 0.4010\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3980 - val_loss: 0.3987\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3966 - val_loss: 0.3930\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3923 - val_loss: 0.3918\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3897 - val_loss: 0.3876\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3872 - val_loss: 0.3871\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3847 - val_loss: 0.3882\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3823 - val_loss: 0.3850\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3803 - val_loss: 0.3820\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3796 - val_loss: 0.3811\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3772 - val_loss: 0.3788\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3756 - val_loss: 0.3788\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3745 - val_loss: 0.3757\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3732 - val_loss: 0.3745\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3723 - val_loss: 0.3744\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3712 - val_loss: 0.3719\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3699 - val_loss: 0.3729\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3694 - val_loss: 0.3733\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3681 - val_loss: 0.3746\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3672 - val_loss: 0.3694\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3667 - val_loss: 0.3694\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3658 - val_loss: 0.3677\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3653 - val_loss: 0.3685\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3645 - val_loss: 0.3665\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3638 - val_loss: 0.3646\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3631 - val_loss: 0.3662\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3625 - val_loss: 0.3669\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3620 - val_loss: 0.3649\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3608 - val_loss: 0.3648\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3622 - val_loss: 0.3620\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3601 - val_loss: 0.3647\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3599 - val_loss: 0.3601\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3597 - val_loss: 0.3597\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3588 - val_loss: 0.3622\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3584 - val_loss: 0.3586\n",
      "Epoch 49/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3575 - val_loss: 0.3635\n",
      "Epoch 50/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3570 - val_loss: 0.3595\n",
      "Epoch 51/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3575 - val_loss: 0.3592\n",
      "Epoch 52/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3556 - val_loss: 0.3603\n",
      "Epoch 53/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3554 - val_loss: 0.3581\n",
      "Epoch 54/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3565 - val_loss: 0.3593\n",
      "Epoch 55/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3556 - val_loss: 0.3569\n",
      "Epoch 56/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3606 - val_loss: 0.3548\n",
      "Epoch 57/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3541 - val_loss: 0.3564\n",
      "Epoch 58/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3533 - val_loss: 0.3562\n",
      "Epoch 59/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3525 - val_loss: 0.3611\n",
      "Epoch 60/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3519 - val_loss: 0.3531\n",
      "Epoch 61/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3517 - val_loss: 0.3535\n",
      "Epoch 62/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3507 - val_loss: 0.3534\n",
      "Epoch 63/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3511 - val_loss: 0.3529\n",
      "Epoch 64/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3530 - val_loss: 0.3636\n",
      "Epoch 65/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3502 - val_loss: 0.3581\n",
      "Epoch 66/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3509 - val_loss: 0.3511\n",
      "Epoch 67/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3486 - val_loss: 0.3523\n",
      "Epoch 68/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3475 - val_loss: 0.3481\n",
      "Epoch 69/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3484 - val_loss: 0.3524\n",
      "Epoch 70/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3476 - val_loss: 0.3504\n",
      "Epoch 71/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3467 - val_loss: 0.3482\n",
      "Epoch 72/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3459 - val_loss: 0.3487\n",
      "Epoch 73/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3454 - val_loss: 0.3506\n",
      "Epoch 74/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3451 - val_loss: 0.3493\n",
      "Epoch 75/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3443 - val_loss: 0.3580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3437 - val_loss: 0.3483\n",
      "Epoch 77/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3430 - val_loss: 0.3461\n",
      "Epoch 78/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3439 - val_loss: 0.3463\n",
      "Epoch 79/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3432 - val_loss: 0.3460\n",
      "Epoch 80/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3413 - val_loss: 0.3479\n",
      "Epoch 81/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3424 - val_loss: 0.3441\n",
      "Epoch 82/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3405 - val_loss: 0.3468\n",
      "Epoch 83/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3399 - val_loss: 0.3509\n",
      "Epoch 84/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3415 - val_loss: 0.3481\n",
      "Epoch 85/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3391 - val_loss: 0.3468\n",
      "Epoch 86/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3383 - val_loss: 0.3441\n",
      "Epoch 87/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3377 - val_loss: 0.3411\n",
      "Epoch 88/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3380 - val_loss: 0.3468\n",
      "Epoch 89/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3383 - val_loss: 0.3439\n",
      "Epoch 90/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3374 - val_loss: 0.3422\n",
      "Epoch 91/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3370 - val_loss: 0.3404\n",
      "Epoch 92/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3367 - val_loss: 0.3411\n",
      "Epoch 93/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3366 - val_loss: 0.3418\n",
      "Epoch 94/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3349 - val_loss: 0.3425\n",
      "Epoch 95/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3346 - val_loss: 0.3394\n",
      "Epoch 96/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3346 - val_loss: 0.3395\n",
      "Epoch 97/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3345 - val_loss: 0.3403\n",
      "Epoch 98/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3361 - val_loss: 0.3397\n",
      "Epoch 99/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3344 - val_loss: 0.3422\n",
      "Epoch 100/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3326 - val_loss: 0.3367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2da8112910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "              validation_data=(X_val, y_val),\n",
    "              callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 26us/sample - loss: 0.3559\n",
      "-0.35591910227324614\n"
     ]
    }
   ],
   "source": [
    "# 사이킷런은 손실이 아니라 점수를 계산하므로, score()의 출력은 음수의 MSE가 됨\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "print(mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6871035  0.8525628  0.86632776] [2.45  0.661 0.842]\n"
     ]
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_pred = keras_reg.predict(X_new)\n",
    "print(y_pred.ravel(), y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 문제의 경우, 하이퍼파라미터가 많으므로 그리드 서치가 아닌 랜덤 탐색을 수행하는 것이 좋다.\n",
    "\n",
    "hidden layer의 수, neuron의 수, learning rate를 사용해 하이퍼파라미터 탐색을 수행해보자.\n",
    "\n",
    "***(참고) 아래 코드는 실행 시, 에러가 나는데 이는 사이킷런 버전에 따른 버그이다.***\n",
    "\n",
    "***https://github.com/ageron/handson-ml2/issues/103 에서도 같은 버그를 겪은 사람이 저자의 깃허브에 issue를 하였다.(사이킷런의 버전을 0.21.3으로 downgrade하면 된다는 이야기도 있는것 같은데 나는 넘어가기로 하였다.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = [{\n",
    "    \"n_hidden\" : [0, 1, 2, 3],\n",
    "    \"n_neurons\" : np.arange(1, 100),\n",
    "    \"learning_rate\" : (reciprocal(3e-4, 3e-2)),\n",
    "}]\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=1, cv=2, verbose=2)\n",
    "# RandomizedSearchCV에서는 k-fold cross validation을 사용하므로,\n",
    "# X_val와 y_val는 사용되지 않으며, 이는 EarlyStopping에만 사용된다.\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 랜덤탐색이 찾은 최상의 하이퍼파라미터로 훈련된 케라스 모델을 가져와보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최상의 하이퍼파라미터와 점수\n",
    "print(rnd_search_cv.best_params_)\n",
    "print(rnd_search_cv.best_score_)\n",
    "\n",
    "# 최상의 모델\n",
    "model = rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 위 모델을 저장하고 test set에서 평가하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 위의 과정은 매우 오랜 시간이 소요되며, 최상의 방법이 아니다.\n",
    "\n",
    "더욱 효율적으로 하이퍼파라미터 공간을 탐색하는 방법은 탐색 지역이 좋을 경우, 그 공간에서 더 탐색을 수행하는 것이다.\n",
    "\n",
    "다음은 하이퍼파라미터 최적화에 사용할 수 있는 몇개의 파이썬 라이브러리들이다.\n",
    "- Hyperopt\n",
    "- Hyperas, kopt, Talos\n",
    "- Keras Tuner\n",
    "- Scikit-Optimize (skopt)\n",
    "- Spearmint\n",
    "- Hyperband\n",
    "- Sklearn-Deap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 은닉층 개수\n",
    "\n",
    "여러 층을 계층적으로 쌓은 심층 신경망은 얕은 신경망보다 파라미터 효율성(parameter efficiency)이 더욱 좋다.\n",
    "- 얕은 신경망보다 적은 수의 뉴런을 사용해 복잡한 함수를 모델링 할 수 있음\n",
    "- 따라서, 더 빠른 학습이 가능함\n",
    "\n",
    "심층 신경망의 hidden layer의 계층별 특징은 다음과 같다.\n",
    "- 아래쪽의 hidden layer : 저수준의 구조를 모델링(예를 들어, 여러 방향이나 모양의 선 등)\n",
    "- 중간쪽의 hidden layer : 저수준의 구조를 연결해 중간수준의 구조를 모델링(예를 들어, 사각형이나 원 등)\n",
    "- 가장 위쪽의 hidden layer : 중간 수준의 구조를 연결해 고수준의 구조를 모델링(예를 들어, 얼굴 등)\n",
    "\n",
    "요약하자면, 계층적 구조의 DNN은 좋은 솔루션으로 빨리 수렴하는 것에 도움이 되며, 새로운 데이터셋으로의 일반화 성능 향상에도 도움이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 은닉 층의 뉴런 개수\n",
    "\n",
    "적절한 뉴런 수를 설정하는 간단한 방법은 overfitting이 일어나기 전까지 점진적으로 뉴런 수를 늘리는 방법이 있다.\n",
    "\n",
    "하지만 이보다 더욱 효과적인 방법은 **실제 필요한 것보다 많은 층과 뉴런을 가진 모델을 선택하고, overfitting되지 않도록 early stopping이나 regularization을 사용**하는 것이다.(이를 stretch pants 방식이라고 함)\n",
    "\n",
    "*참고로 층의 뉴런 수보다 층 수를 늘리는 것이 더욱 이득이 많다고 함*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습률, 배치 크기, 다른 하이퍼파라미터들\n",
    "\n",
    "### 학습률\n",
    "낮은 학습률에서 시작해서 점진적으로 높여가며 반복해 모델을 훈련한다. 처음에는 학습률이 커짐에 따라 손실이 줄어들지만, 학습률이 어느 지점보다 커지면 다시 손실이 커질 것이다.\n",
    "\n",
    "이 경우, 최적의 학습률은 손실이 다시 상승하는 지점보다 조금 아래에 있을 것이다.(일반적으로 상승점보다 약 10배 낮은 지점)\n",
    "\n",
    "*최적의 학습률은 다른 하이퍼파라미터(특히 배치 크기)에 의존적이므로, 다른 하이퍼파라미터가 수정되면 학습률도 반드시 튜닝해야 한다.*\n",
    "\n",
    "### 옵티마이저\n",
    "평범한 미니배치 경사 하강법이 아닌 더 좋은 옵티마이저를 선택하고, 이 옵티마이저의 하이퍼 파라미터를 튜닝할 수 있다.\n",
    "\n",
    "### 배치 크기\n",
    "큰 배치는 학습 시간을 크게 단축시킬 수 있지만, 일반화 성능에 영향을 미치므로 작은 배치(2~32)를 사용해야 한다는 논문도 있다.\n",
    "\n",
    "하지만 반대로 학습률 예열(작은 학습률에서 시작해 점점 높이는 방식)과 같은 다양한 기법을 사용하면 매우 큰 배치크기(8192까지)도 사용할 수 있다는 논문도 있다.\n",
    "\n",
    "따라서, 학습률 예열을 사용해 큰 배치 크기를 시도해보고, 훈련이 불안정하거나 성능이 좋지 못하다면 작은 배치 크기를 사용하는 것이 좋다.\n",
    "\n",
    "### 활성화 함수\n",
    "일반적으로 ReLU가 hidden layer에 사용하기 좋은 기본 활성화 함수이다.\n",
    "\n",
    "출력층의 활성화 함수는 수행하는 작업에 따라 다르다.\n",
    "\n",
    "### 반복 횟수\n",
    "대부분의 경우, 반복횟수는 튜닝할 필요가 없으며 대신 early stopping을 사용하면 된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
