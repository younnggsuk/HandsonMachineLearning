{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 생물학적 뉴런에서 인공 뉴런까지\n",
    "\n",
    "## 인공 신경망 (Artificial Neural Network, ANN)\n",
    "인공신경망은 뇌에 있는 생물학적 뉴런의 네트워크에서 영감을 받은 머신러닝 모델을 의미한다. 이는 딥러닝의 핵심이며, 다재다능하고 강력하고 확장성이 좋아서 아주 복잡한 대규모 머신러닝 문제를 다루는데 적합하다.\n",
    "\n",
    "## 퍼셉트론 (Perceptron)\n",
    "퍼셉트론은 간단한 인공 신경망 구조 중 하나로 LTU(linear threshold unit)라 불리는 인공뉴런을 기반으로 한다.\n",
    "\n",
    "LTU는 입력의 가중치 합($z = w_1x_1 + w_2x_2 + \\cdots + w_nx_n = \\mathbf{x}^T\\mathbf{w}$)을 계산하고, 여기에 **unit step function**를 적용하여 다음과 같이 결과를 출력한다.\n",
    "\n",
    "$$h_w(\\mathbf{x}) = step(z) = \\begin{equation} \\begin{cases}0 & z<0\\text{일때} \\\\ 1 & z \\ge 0 \\text{일때} \\end{cases} \\end{equation}$$\n",
    "\n",
    "아래 그림은 LTU를 나타낸 그림이다.\n",
    "\n",
    "<img src=\"./images/ltu.png\" alt=\"ltu\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나의 LTU는 입력의 선형 조합을 계산해, 그 결과가 임곗값을 넘으면 양성, 그렇지 않으면 음성 클래스를 출력하는 방식으로 간단한 선형 이진 분류 문제에 사용할 수 있다.\n",
    "\n",
    "보통 입력 뉴런에는 항상 1을 출력하는 편향(bias) 뉴런이 포함되며, 출력의 수에 따라 서로 다른 이진 클래스를 동시에 분류하는 다중 출력 분류기(multioutput classifier)가 될 수 있다. 아래 그림은 입력 2개와 출력 3개로 구성된 퍼셉트론이다.\n",
    "\n",
    "<img src=\"./images/multioutput_perceptron.png\" alt=\"multioutput_perceptron\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 경우, 선형대수를 사용해 다음과 같이 여러 샘플에 대한 출력을 계산할 수 있다.\n",
    "\n",
    "$$h_{\\mathbf{W}, b}(\\mathbf{X}) = \\phi(\\mathbf{XW} + \\mathbf{b})$$\n",
    "- $\\mathbf{X}$ : 입력 특성의 행렬(행은 샘플, 열은 특성)\n",
    "- $\\mathbf{W}$ : bias를 제외한 모든 가중치들의 행렬(행은 입력 뉴런, 열은 출력 뉴런)\n",
    "- $\\mathbf{b}$ : bias로 구성된 벡터\n",
    "- $\\phi$ : 활성화 함수(여기서는 LTU이므로, step function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론의 학습\n",
    "퍼셉트론의 학습 규칙을 식으로 나타내면 다음과 같다.\n",
    "$$w_{i, j}^{(next step)} = w_{i, j} + \\eta (y_i - \\hat{y}_i) x_i$$\n",
    "- $w_{i, j}$ : $i$번째 입력 뉴런과 $j$번째 출력 뉴런 사이의 가중치\n",
    "- $x_{i}$ : 현재 훈련 샘플의 $i$번째 입력 뉴런의 입력값\n",
    "- $\\hat{y}_i$ : 현재 훈련 샘플의 $j$번째 출력 뉴런의 출력값\n",
    "- $y_i$ : 현재 훈련 샘플의 $j$번째 출력 뉴런의 타깃값\n",
    "- $\\eta$ : learning rate\n",
    "\n",
    "즉, 현재의 출력과 타깃의 차이(오차)를 통해 가중치를 업데이트 한다는 의미이다.\n",
    "\n",
    "훈련 샘플이 선형적으로 구분될 수 있다면, 위의 알고리즘으로 정답에 수렴할 수 있다. 이를 **퍼셉트론 수렴이론(perceptron convergence theory)**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런은 하나의 LTU layer로 구성된 `Perceptron`클래스를 제공한다. 다음은 이를 이용해 붓꽃 데이터셋에 적용한 예이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론의 약점\n",
    "퍼셉트론의 약점은 XOR 분류 문제와 같은 일부 간단한 문제를 풀 수 없다는 것이다. 이는 로지스틱 회귀와 같은 다른 선형 분류기도 마찬가지이다.\n",
    "\n",
    "이와 같은 약점은 퍼셉트론을 여러 개 쌓아올리면 해결할 수 있는데, 이를 다층 퍼셉트론(Multi-Layer Perceptron, MLP)라고 한다.\n",
    "\n",
    "다음 그림은 MLP로 XOR 문제를 푸는 것을 나타낸다.(입력이 (1, 1), (0, 0)일 때 0을 출력하고, (1, 0), (0, 1)일때 1을 출력함)\n",
    "\n",
    "<img src=\"./images/mlp.png\" alt=\"mlp\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다층 퍼셉트론 (Multi-Layer Perceptron, MLP)과 역전파 (Backpropagation)\n",
    "\n",
    "다층 퍼셉트론은 하나의 입력층(input layer)과 은닉층(hidden layer)이라 부르는 여러 LTU층, 그리고 출력층(output layer)로 구성되며, 신호가 입력에서 출력으로만 흐르는 피드포워드 신경망(feedforward neural network)에 속한다.\n",
    "\n",
    "다음 그림은 MLP의 예이다.\n",
    "\n",
    "<img src=\"./images/mlp_2.png\" alt=\"mlp_2\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 심층 신경망 (Deep Neural Network, DNN)\n",
    "hidden layer를 여러 개 쌓아 올린 인공 신경망을 심층 신경망이라고 한다. 딥러닝은 심층 신경망을 연구하는 분야이며, 연산이 연속하여 길게 연결돤 모델을 연구하는 분야이다.\n",
    "\n",
    "\n",
    "### 역전파 (Backpropagation)\n",
    "역전파는 다층 퍼셉트론이나 DNN과 같은 네트워크를 훈련하는데 사용하는 알고리즘이다. (step function은 gradient가 없으므로, 다른 활성화 함수로 대체하여야 함)\n",
    "\n",
    "역전파는 네트워크를 2번(정방향, 역방향 한번씩) 통과하며, 모든 모델 파라미터에 대한 오차의 gradient를 계산하고, 이를 통해 경사 하강법을 수행한다.\n",
    "\n",
    "역전파 알고리즘의 동작을 더욱 자세히 살펴보면 다음과 같다.\n",
    "1. 각 훈련 샘플에 대해 첫번째 은닉층에서부터 마지막 출력층까지의 모든 출력을 계산한다.(정방향 계산)\n",
    "2. 손실함수를 사용해 네트워크의 출력 오차를 측정한다.\n",
    "3. 출력층에서부터 첫번째 입력층까지 역방향으로 각 층을 거치면서 연결 가중치가 오차의 기여 정도에 얼마나 기여했는지 측정한다.(오차의 gradient 계산, chain rule 사용)\n",
    "4. 경사하강법으로 오차가 감소하도록 가중치를 조정한다.\n",
    "\n",
    "역전파 알고리즘이 동작하기  또한, gradient가 있는 활성화 함수를 사용해야 한다.\n",
    "\n",
    "여러 아래 그림은 여러 활성화 함수과 미분을 나타낸 것이다.\n",
    "\n",
    "<img src=\"./images/activation_functions_and_derivatives.png\" alt=\"activation_functions_and_derivatives\">\n",
    "\n",
    "활성화 함수를 사용하는 이유는, 층 사이에 비선형성을 추가해주어 복잡한 문제를 풀 수 있도록 해주기 때문이다. 비선형 활성화 함수를 사용하는 DNN은 이론적으로 어떠한 연속함수도 근사할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀를 위한 다층 퍼셉트론\n",
    "다층 퍼셉트론은 회귀 문제를 해결하는데 사용할 수 있다. 회귀 MLP의 전형적인 구조는 다음과 같다.\n",
    "\n",
    "|하이퍼 파라미터|일반적인 값|\n",
    "|:------------|:---------|\n",
    "|입력 뉴런 수|특성마다 하나 (예를 들어 MNIST의 경우 28x28 = 784)|\n",
    "|은닉층 수|문제에 따라 다르지만, 일반적으로 1~5|\n",
    "|은닉층의 뉴런 수|문제에 따라 다르지만, 일반적으로 10~100 사이|\n",
    "|은닉층의 활성화 함수|예측 차원마다 1개|\n",
    "|출력 뉴런 수|ReLU(또는 SELU)|\n",
    "|출력층의 활성화 함수|일반적으로는 없음<br />출력이 양수인 경우, ReLU/softplus<br />출력을 특정 범위로 제한할 때, logistic/tanh|\n",
    "|손실 함수|일반적으로는 MSE<br />이상치가 있는 경우, MAE / Huber|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류를 위한 다층 퍼셉트론\n",
    "다층 퍼셉트론은 분류 작업에도 사용할 수 있다. 분류 MLP의 전형적인 구조는 다음과 같다.\n",
    "\n",
    "|하이퍼 파라미터|이진 분류|다중 레이블 분류|다중 분류|\n",
    "|:-----------|:------|:------------|:------|\n",
    "|입력층과 은닉층|회귀와 동일|회귀와 동일|회귀와 동일|\n",
    "|출력 뉴런 수|1개|레이블마다 1개|클래스마다 1개|\n",
    "|출력층의 활성화 함수|로지스틱 함수|로지스틱 함수|소프트맥스 함수|\n",
    "|손실 함수|크로스 엔트로피|크로스 엔트로피|크로스 엔트로피|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
