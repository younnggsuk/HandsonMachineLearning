{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Hyperparameters\n",
    "\n",
    "Decition tree는 training data에 대한 가정을 거의 하지 않는다.(반대로 linear model은 data가 linear하다고 가정한다.) 이와 같이 가정을 하지 않으면, data에 더욱 fit하게 되므로 overfitting이 일어나기 쉽다.\n",
    "\n",
    "tree와 같이 training data에 대한 가정을 하지 않는 model을 **nonparametric model**이라고 한다. 이는 parameter가 없어서가 아니라 training전에 parameter의 수가 결정되지 않기 때문에 붙은 이름이다. 따라서, model 구조가 data에 맞춰서 고정되지 않고 자유로운 형태이다.\n",
    "\n",
    "반대로 linear model처럼 training data에 대한 가정을 하는 model을 **parametric model**이라고 한다. 이는 training 전에 결정된 파라미터의 수가 많으며, 자유도가 제한되며, overfitting의 위험이 줄어든다.(하지만 반대로 underfitting되기 쉬움)\n",
    "\n",
    "Decision Tree에서 overfitting을 막기 위해서는 model의 자유도를 제한해야 하며, 이는 regularization을 통해 수행할 수 있다. regularization hyperparameter는 사용하는 알고리즘에 따라 다르지만, 일반적으로는 적어도 maximum depth는 제한할 수 있다. 사이킷런에서는 `max_depth` hyperparameter를 통해 이를 제한한다. default 값은 `None`으로 maximum depth의 제한이 없는 것을 의미하며, `max_depth`를 줄이면 overfitting의 위험이 감소하게 된다.\n",
    "\n",
    "`DecisionTreeClassifier`에는 (`max_depth`외에)decision tree를 제한하는 여러 parameter들이 있다.\n",
    "- `min_samples_split` : 분할되기 위해 node가 가져야 하는 최소 sample 수\n",
    "- `min_samples_leaf` : leaf node가 반드시 가져야 하는 최소 sample 수\n",
    "- `min_weight_fraction_leaf` : leaf node가 반드시 가져야 하는 전체 sample수에 대한 weighted sample의 비율\n",
    "- `max_leaf_nodes` : leaf node의 최대 수\n",
    "- `max_features` : 각 node의 분할에 사용되는 최대 feature의 수\n",
    "\n",
    "위의 hyperparameter에서, `min_*`를 증가시키고, `max_*`를 감소시키면, model을 regularize하게 된다.\n",
    "\n",
    "아래 그림은 moons dataset으로 학습한 2개의 decision tree를 나타낸 것이다. 왼쪽의 경우는 regularization을 적용하지 않았고, 오른쪽의 경우는 `min_samples_leaf=4`를 지정한 것이다.\n",
    "\n",
    "<br/>\n",
    "<img src=\"./images/decision_tree_regularization.png\" alt=\"decision_tree_regularization\">\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "purity를 증가시키는 것이 **statistically significant**하지 않다면, leaf node 바로 위의 node는 불필요할 수 있다. 이에 기반하여 제한 없이 decision tree를 학습시킨 후 불필요한 node를 **pruning**하는 알고리즘도 있다. \n",
    "\n",
    "이 알고리즘은 **$\\chi^2$ test(chi-squared test)**와 같은 통계적 검정 방법을 사용하여 purity의 향상이 우연히 일어난 것인지(이를 **null hypothesis**라고 부름)를 추정한다. 추정된 이 확률을 **p-value**라고 하며, 이 값이 threshold보다 높으면 해당 노드를 불필요하다고 간주하고 삭제하는 방식이다. 이는 불필요한 노드가 모두 없어질 때까지 pruning을 계속한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
