{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions\n",
    "\n",
    "앞의 예제 결과에서, decision tree가 어떻게 prediction을 하는지 살펴보자. \n",
    "\n",
    "<br/>\n",
    "<img src=\"./images/iris_tree.png\" alt=\"iris_tree\">\n",
    "<br/>\n",
    "\n",
    "먼저, **root node**(depth 0, top)에서 petal length가 2.45cm보다 작은지를 검사한다. 만약, 작다면 root node의 왼쪽 child node(depth 1, left)로 이동한다. 이 경우에는, 이 노드가 **leaf node**이므로, Decision Tree는 `class=setosa`라고 predict한다. 만약, 크다면 root node의 오른쪽 child node(depth 1, right)로 이동한다. 이 경우는 leaf node가 아니므로, 다시 한번 petal width가 1.75cm보다 작은지를 검사한다. 만약, 작다면 왼쪽 child node로 이동하여 `class=versicolor`, 크다면 오른쪽 child node로 이동하여 `class=virginica`라고 predict하게 된다.\n",
    "\n",
    "node의 `samples`는 training instance가 얼마나 적용되었는지를 나타낸다. 예를 들어 위 그림에서는, petal length가 2.45보다 큰 instance는 100개가 있고, 그 중에서 petal width가 1.75보다 작은 instance는 54개가 있다.\n",
    "\n",
    "node의 `value`는 각 class별 training instance의 수를 나타낸다. 맨 오른쪽 아래의 node는 [setosa, versicolor, virginica]가 각각 [0, 1, 45]개씩 있다.\n",
    "\n",
    "node의 `gini`는 **impurity**를 나타낸다. 모든 training instance가 같은 class를 가질 때, \"pure\"하다고 하며, `gini=0`이 된다. 예를 들어, depth 1, left node의 경우, class가 setosa인 instance뿐이므로, `gini=0`이다.\n",
    "\n",
    "다음의 식은 $i$번째 node의 `gini` score $G_i$를 계산하는 식이다.\n",
    "\n",
    "**Gini impurity**\n",
    "\n",
    "$G_i = 1 - \\sum^n_{k=1} p_{i, k}^2$\n",
    "\n",
    "- $p_{i, k}$는 i번째 node에 있는 class k의 instance 비율\n",
    "\n",
    "위 식을 통해 depth 2, left node의 gini impurity를 계산해보면, 총 54개의 sample에서 class별로 0, 49, 5개씩 있으므로, $1-\\big((\\frac{0}{54})^2 + (\\frac{49}{54})^2 + (\\frac{5}{54})^2\\big) \\approx 0.168$이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<img src=\"./images/decision_tree_boundary.png\" alt=\"decision_tree_boundary\">\n",
    "<br/>\n",
    "\n",
    "위 그림은 여기서 만든 decision tree의 decision boundary를 보여준다.\n",
    "\n",
    "굵은 veritical line이 root node의 decision boundary(petal length=2.45cm)를 나타내고, dash line은 depth-1 right node의 boundary(petal width=1.75cm)이다.\n",
    "\n",
    "`max_depth`를 2로 설정하였으므로 여기서 멈추었지만, 만약 3으로 설정하였다면 dotted line과 같은 decision boundary가 추가되었을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation : White Box vs. Black Box\n",
    "\n",
    "Decision tree는 직관적이며, 어떻게 결정되었는지 이해하기 쉽다. 이러한 모델을 **white box model**이라고 한다.\n",
    "\n",
    "반면, 이후의 챕터에서 보게될 Random Forests나 Neural Networks는 **black box model**이라고 한다. 이 알고리즘들은 성능이 뛰어나고, prediction을 만드는 연산과정을 쉽게 확인할 수 있지만, 왜 그런 prediction을 만들었는지는 쉽게 설명하기 어렵다.\n",
    "\n",
    "예를 들어, Neural Network가 어떤 사진에 사람이 있다고 판단했을 때, 모델이 사람의 눈을 인식한 것인지 아니면 입이나 코, 신발 또는 사람이 앉아있던 소파를 인식한 것인지 알기가 어렵다. 하지만 Decision Tree는 직접 따라해볼 수도 있는 간단하고 명확한 classification rule을 사용한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
