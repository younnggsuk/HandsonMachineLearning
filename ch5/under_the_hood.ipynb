{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under the Hood\n",
    "\n",
    "이번 챕터에서는 SVM이 어떻게 prediction을 하고, 알고리즘이 어떻게 동작하는지에 대해 살펴본다.\n",
    "\n",
    "SVM을 다룰 때에는 다음과 같이 표기법을 다르게 사용할 것이다.\n",
    "- bias term : $\\theta_0$ -> $b$\n",
    "- input feature vector : $\\theta_1$~$\\theta_n$ -> $\\mathbf{w}$\n",
    "\n",
    "따라서, 정리하면 model의 parameter vector $\\mathbf{\\theta}$를 $\\mathbf{w}$+b로 나타낸다는 것이다.\n",
    "\n",
    "$\\therefore \\mathbf{\\theta} = \\mathbf{w} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Function and Predictions\n",
    "\n",
    "linear SVM classifier model은 다음의 decision function을 통해 새로운 instance $\\mathbf{x}$의 class를 예측한다. 만약, 결과가 positive라면, 예측된 class $\\hat{y}$는 positive class (1)일 것이고, 아니라면, negative class (0)일 것이다.\n",
    "\n",
    "**Linear SVM classifier prediction**\n",
    "\n",
    "$\\hat{y} = \\begin{equation} \\begin{cases} 0 & \\text{if} & \\mathbf{w}^T \\mathbf{x} + b < 0 \\\\  1 & \\text{if} & \\mathbf{w}^T \\mathbf{x} + b \\ge 0\\end{cases} \\end{equation}$\n",
    "\n",
    "\n",
    "아래 그림은 앞절에서 구현한 iris-dataset에 대한 SVM classifier model에 decision function을 같이 나타낸 것이다. \n",
    "\n",
    "<br/>\n",
    "<img src=\"./images/decision_function_for_the_iris_dataset.png\" alt=\"decision_function_for_the_iris_dataset\"> \n",
    "<br/>\n",
    "\n",
    "위 그림에서 decision boundary는 decision function 평면이 0이 되는 직선이다. dashed line은 decision function이 각각 1과 -1이 되는 직선이며, 이 두 직선은 평행하고, decision boundary에서 일정한 간격만큼 떨어져 margin을 만든다.\n",
    "\n",
    "따라서, SVM classifier를 학습시킨다는 것은, margin violation이 없거나(hard margin) 또는 제한하여(soft margin) 최대한 넓은 margin을 만드는 decision function $\\mathbf{x}$와 $b$를 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objective\n",
    "\n",
    "Decision function의 기울기는 weight vector의 norm $\\|\\mathbf{w}\\|$과 같다. 만약, 이 값을 2로 나눈다면, 다음 그림과 같이 decision function의 +1, -1지점은 2배 더 멀어질 것이다. 이는 slope를 2로 나누면, margin이 2배 더 커진다는 뜻이며, 이를 통해 weight vector가 작아질수록, $\\mathbf{w}$는 margin이 커진다는 것을 알 수 있다.\n",
    "\n",
    "<br/>\n",
    "<img src=\"./images/smaller_weight_vector_larger_margin.png\" alt=\"smaller_weight_vector_larger_margin\"> \n",
    "<br/>\n",
    "\n",
    "만약 hard margin이라고 가정한다면, decision function이 모든 positive instance에 대해서는 1보다 커야하고, negative instance에 대해서는 -1보다 작아야한다. negative instance($y^{(i)}=0$)일때 $t^{(i)}=-1$, positive instance($y^{(i)}=1$)일때 $t^{(i)}=1$라고 한다면, 모든 instance에 대한 hard margin의 제약조건은 $t^{(i)}(\\mathbf{w}^T\\mathbf{x}^{(i)} + b) \\ge 1$로 나타낼 수 있다.\n",
    "\n",
    "따라서, hard margin linear SVM classifier의 objective는 다음의 constrained optimization 문제로 나타낼 수 있다.\n",
    "\n",
    "**Hard margin linear SVM classifier objective**\n",
    "\n",
    "$\\text{minimize} \\dfrac{1}{2} \\mathbf{w}^T\\mathbf{w}$ \n",
    "- $\\text{subject to } t^{(i)}(\\mathbf{w}^T\\mathbf{x}^{(i)} + b) \\ge 1 \\text{ for } i=1, 2, \\cdots, m$\n",
    "\n",
    "위 식에서, $\\| \\mathbf{w} \\|$가 아닌 $\\dfrac{1}{2}\\| \\mathbf{w} \\|^2\\big(=\\dfrac{1}{2}\\mathbf{w}^T\\mathbf{x}\\big)$를 minimize하는 이유는 $\\dfrac{1}{2}\\| \\mathbf{w} \\|^2$는 쉽게 미분이 가능하지만($\\mathbf{w}$가 됨), $\\| \\mathbf{w} \\|$는 0에서 미분이 되지 않기 때문이다.\n",
    "\n",
    "soft margin의 objective를 구하기 위해서는, 각 instance에 대한 **slack variable** $\\zeta^{(i)} \\ge 0$를 도입해야 한다. $\\zeta^{(i)}$는 i번째 instance가 margin을 얼마나 violate할지를 정하는 값이다.\n",
    "\n",
    "이제 slack variable을 가능한 작게 해서 margin violation을 줄이면서, 동시에 $\\dfrac{1}{2} \\mathbf{w}^T\\mathbf{w}$를 가능한 작게 해서 margin을 줄여야 하는 서로 상충되는 두 objective가 생겼다. 여기서 두 objective간의 tradeoff를 정의하는 C라는 값을 정의한다.(hyperparameter `C`)\n",
    "\n",
    "위 내용을 정리하면, soft margin linear SVM classifier의 objective는 다음의 constrained optimization 문제로 나타낼 수 있다.\n",
    "\n",
    "**Soft margin linear SVM classifier objective**\n",
    "\n",
    "$\\text{minimize} \\dfrac{1}{2} \\mathbf{w}^T\\mathbf{w} + C \\sum^m_{i=1} \\zeta^{(i)}$\n",
    "- $\\text{subject to } t^{(i)}(\\mathbf{w}^T\\mathbf{x}^{(i)} + b) \\ge 1 - \\zeta^{(i)} \\text{ and } \\zeta^{(i)} \\ge 0 \\text{ for } i=1, 2, \\cdots, m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(이후 설명되는 내용들은 모두 convex optimization과 관련된 내용들이라서 나중에 따로 공부하기)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
