{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Challenges of Machine Learning\n",
    "\n",
    "머신러닝에서 문제가 되는 2가지는 \"bad algorithm\"과 \"bad data\"이다.\n",
    "\n",
    "## 1. Bad Data\n",
    "\n",
    "### Insufficient Quality of Training Data\n",
    "\n",
    "대부분의 머신러닝 알고리즘이 제대로 동작하기 위해서는 매우 많은 양의 데이터가 필요하다.\n",
    "\n",
    "아래 그래프는 자연어 처리에서 데이터 양에 따른 여러가지 머신러닝 알고리즘의 성능을 나타낸 것이다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./images/importance_of_data_versus_algorithms.png\" alt=\"importance_of_data_versus_algorithms\" style=\"width:60%\"> \n",
    "    <br/>\n",
    "    <figcaption><center><i>The importance of data versus algorithms</i></center></figcaption>\n",
    "</figure>\n",
    "\n",
    "<br/>\n",
    "\n",
    "결과를 보면 데이터 양이 커짐에 따라 모두 비슷한 성능을 내고 있다는 것을 알 수 있는데, 여기서 주목할 점은 사용된 알고리즘에는 아주 단순한 알고리즘도 포함되어 있다는 것이다.\n",
    "\n",
    "이는 알고리즘의 성능이 떨어지더라도 많은 데이터가 있다면 좋은 성능을 낼 수 있다는 점과 데이터의 양이 매우 중요하다는 것을 알 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonrepresentative Training Data\n",
    "\n",
    "일반화가 잘 되기 위해서는, 학습 데이터가 일반화 하기 원하는 새로운 데이터에 representative 하는 것이 중요하다. 이는 instance-based learning과 model-based learning 모두에 해당한다.\n",
    "\n",
    "아래 그래프는 1인당 GDP에 따른 삶 만족도에 대한 linear 모델을 나타낸 것이다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./images/a_more_representative_training_sample.png\" alt=\"a_more_representative_training_sample\" > \n",
    "    <br/>\n",
    "    <figcaption><center><i>A more representative training sample</i></center></figcaption>\n",
    "</figure>\n",
    "\n",
    "<br/>\n",
    "\n",
    "위 결과를 보면, 파란 점선으로 된 모델은 빨간 점 데이터에 대해 검정 실선 모델에 비해 좋은 성능을 내지 못하는 것을 볼 수 있다. 또 반대로, 검정 실선으로 된 모델은 파란 점 데이터에 대해 파란 점선 모델에 비해 좋은 성능을 내지 못하는 것을 볼 수 있다.\n",
    "\n",
    "따라서 모델의 일반화 성능을 높이기 위해서는 일반화 하고자 하는 대상에 representative한 데이터로 학습 데이터를 구성하는 것이 매우 중요하다. 하지만 이는 쉽지 않다.\n",
    "\n",
    "만약, 데이터에서 sample을 작게 한다면 우연히 nonrepresentative한 데이터만 sample로 사용될 수 있으므로 sampling noise가 발생한다. \n",
    "\n",
    "또한, 데이터가 매우 큰 경우라도 샘플링 방법이 잘못된다면 non-representative할 수 있다. 이 경우를 sampling bias라고 한다.\n",
    "\n",
    "#### Sampling Bias\n",
    "Sampling bias는 전체 데이터에서 특정 sample을 더 높은 확률로 sampling하여 생기는 편향(bias)를 말한다.\n",
    "\n",
    "대표적인 예로는 1936년 Landon과 Roosevelt의 대통령 선거에서 Literary Digest 잡지사가 수행한 여론조사가 있다. 당시 Literary Digest는 여론조사 결과 57%의 확률로 Landon이 당선될 것이라고 예측하였다. 하지만 선거 결과는 62%로 루즈벨트가 당선되었다.\n",
    "\n",
    "잘못된 예측은 Literary Digest가 공화당 지지자일 확률이 높은 사람만을 대상으로 여론조사를 수행하였기 때문이며 이는 잘못된 샘플링 방법에 의한 sampling bias의 한 예이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poor-Quality Data\n",
    "\n",
    "먄약, 학습 데이터에 측정 방식의 문제로 인해 error, outlier(이상치), noise가 많다면, 머신러닝 시스템이 패턴을 감지하기 어려울 것이며, 성능 저하로 이어질 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irrelevant Features\n",
    "\n",
    "머신러닝 프로젝트가 성공하기 위해서는 좋은 feature로 이루어진 training set이 학습에 사용되어야 한다. 이를 위한 과정을 feature engineering이라고 한다.\n",
    "\n",
    "Feature engineering은 다음을 포함한다.\n",
    "- Feature selection : 유용한 feature만을 선택하기\n",
    "- Feature extraction : feature를 합쳐서 더 유용한 하나의 feature로 만들기\n",
    "- 새로운 데이터를 수집해 새로운 feature를 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bad Algorithms\n",
    "\n",
    "### Overfitting the Training Data\n",
    "\n",
    "Overfitting은 모델이 training data에 overfit되어 training data에는 잘 동작하지만, (일반화가 잘 되지 않아서) 새로운 데이터에 잘 동작하지 않는 경우를 의미한다.\n",
    "\n",
    "아래 그래프는 앞에서 살펴본 1인당 GDP에 따른 삶 만족도 데이터에 대해 overfitting된 경우이다. 과연 이 모델이 새로운 데이터에 대해 제대로 prediction을 할 수 있을까?\n",
    "\n",
    "<br/>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./images/overfitting.png\" alt=\"overfitting\" > \n",
    "    <br/>\n",
    "    <figcaption><center><i>Overfitting the training data</i></center></figcaption>\n",
    "</figure>\n",
    "\n",
    "<br/>\n",
    "\n",
    "Overfitting은 training data의 양과 노이즈 정도에 비해 모델이 매우 복잡할 때 일어난다. 이를 해결하기 위한 방법은 다음과 같다.\n",
    "- 적은 파라미터를 가지도록 모델을 단순화하기 (고차원 모델 대신 저차원 모델 사용 등)\n",
    "- training data 더 모으기\n",
    "- training data의 noise 줄이기 (error 수정 및 outlier 제거 등)\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "모델을 제한해 단순하게 만들어서 overfitting의 위험을 줄이는 방법을 regularization이라고 한다.\n",
    "\n",
    "아래 그래프는 regularization을 통해 overfitting의 위험을 줄인 것을 보여준다.\n",
    "\n",
    "부분 데이터로 학습시킨 모델에 regularization을 적용하여 기울기가 조금 감소해 부분 데이터에 대한 overfitting을 감소시킨 것을 보여준다\n",
    "\n",
    "<br/>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./images/regularization.png\" alt=\"regularization\" > \n",
    "    <br/>\n",
    "    <figcaption><center><i>Regularization reduces the risk of overfitting</i></center></figcaption>\n",
    "</figure>\n",
    "\n",
    "<br/>\n",
    "\n",
    "regularization의 정도는 hyperparameter를 통해 조절한다. hyperparameter는 학습 알고리즘의 파라미터이다.(모델의 파라미터가 아니다) 이는 학습 알고리즘에 영향을 받지 않고 학습 전에 설정되어 학습 중에 값을 유지하는 값이다.\n",
    "\n",
    "따라서 하이퍼파라미터를 튜닝하며 좋은 성능을 내는 솔루션을 찾는 것은 머신러닝 시스템을 만드는 과정에서 중요한 부분이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting the Training Data\n",
    "\n",
    "Underfitting은 모델이 너무 단순해 데이터에 내재된 구조를 학습하지 못하는 경우를 의미한다. overifitting과 반대인 경우이다.\n",
    "\n",
    "Underfitting을 완화하기 위한 방법들은 다음과 같다.\n",
    "- 더 강력한(파라미터가 더 많은) 모델 선택\n",
    "- 더 나은 feature를 주입하기(feature engineering)\n",
    "- 모델의 제한을 줄이기(regularization 파라미터 줄이기 등)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
